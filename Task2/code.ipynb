{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python374jvsc74a57bd097ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "from functions import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "# Random Seed for dataset generation\n",
    "sampling_seed = 15\n",
    "torch.manual_seed(sampling_seed)"
   ]
  },
  {
   "source": [
    "# Variables to CF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_table('TrainingData_401.txt', delimiter=' ', dtype='float', header=None)\n",
    "data = np.array(data.iloc[:,:])\n",
    "\n",
    "perc = 0\n",
    "validation_size = int(perc*data.shape[0]/100)\n",
    "training_size = data.shape[0] - validation_size\n",
    "data_norm, data_train, data_val = [], [], []\n",
    "for i in range(data.shape[1]):\n",
    "    data_norm.append(data[:,i])\n",
    "    data_train.append(data[:training_size,i])\n",
    "    data_val.append(data[training_size:,i])\n",
    "\n",
    "data_mean, data_std = [], []\n",
    "for i in range(data.shape[1]):\n",
    "    data_mean.append(data[:,i].mean())\n",
    "    data_std.append(data[:,i].std())\n",
    "    data_norm[i] = ((data_norm[i]-data_mean[i])/data_std[i]).reshape(-1,1)\n",
    "    data_train[i] = ((data_train[i]-data_mean[i])/data_std[i]).reshape(-1,1)\n",
    "    data_val[i] = ((data_val[i]-data_mean[i])/data_std[i]).reshape(-1,1)"
   ]
  },
  {
   "source": [
    "network_properties = {\n",
    "    \"hidden_layers\": [4, 10, 20],\n",
    "    \"neurons\": [20, 50, 100],\n",
    "    \"regularization_exp\": [2],\n",
    "    \"regularization_param\": [0],\n",
    "    \"batch_size\": [data_norm[0].shape[0]],\n",
    "    \"epochs\": [1000],\n",
    "    \"optimizer\": [\"LBFGS\"],\n",
    "    \"init_weight_seed\": [np.random.randint(0,100)]\n",
    "}\n",
    "\n",
    "settings = list(itertools.product(*network_properties.values()))\n",
    "\n",
    "i = 0\n",
    "\n",
    "train_err_conf = list()\n",
    "val_err_conf = list()\n",
    "test_err_conf = list()\n",
    "for set_num, setup in enumerate(settings):\n",
    "    print(\"###################################\", set_num, \"###################################\")\n",
    "    setup_properties = {\n",
    "        \"hidden_layers\": setup[0],\n",
    "        \"neurons\": setup[1],\n",
    "        \"regularization_exp\": setup[2],\n",
    "        \"regularization_param\": setup[3],\n",
    "        \"batch_size\": setup[4],\n",
    "        \"epochs\": setup[5],\n",
    "        \"optimizer\": setup[6],\n",
    "        \"init_weight_seed\": setup[7]\n",
    "    }\n",
    "\n",
    "    relative_error_train_, relative_error_val_ = run_configuration(setup_properties, torch.tensor(np.concatenate((data_train[:8]),axis=1)).float(), torch.tensor(data_train[8]).float(), 8, 1)\n",
    "    train_err_conf.append(relative_error_train_)\n",
    "    val_err_conf.append(relative_error_val_)\n",
    "\n",
    "print(train_err_conf, val_err_conf)\n",
    "\n",
    "train_err_conf = np.array(train_err_conf)\n",
    "val_err_conf = np.array(val_err_conf)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "plt.scatter(np.log10(train_err_conf), np.log10(val_err_conf), marker=\"*\")\n",
    "plt.xlabel(\"Selection Criterion\")\n",
    "plt.ylabel(\"Generalization Error\")\n",
    "plt.title(r'Validation - Training Error VS Generalization error ($\\sigma=0.0$)')\n",
    "#plt.savefig(\"sigma.png\", dpi=400)\n",
    "plt.show()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = data_norm[0].shape[0]\n",
    "retrain = 128\n",
    "n_epochs_1 = 300\n",
    "training = DataLoader(torch.utils.data.TensorDataset(torch.tensor(np.concatenate((data_train[:8]),axis=1)), torch.tensor(data_train[8])), batch_size=batch_size, shuffle=True)\n",
    "my_network_1 = NeuralNet_Seq(input_dimension=8, output_dimension=1, n_hidden_layers=4, neurons=20)\n",
    "init_xavier(my_network_1, retrain + np.random.randint(-retrain,retrain))\n",
    "optimizer = optim.LBFGS(my_network_1.parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "if perc == 0:\n",
    "    history_1 = fit(my_network_1, training, n_epochs_1, optimizer, p=2, reg_param=0.0, verbose=False)\n",
    "else:\n",
    "    history_1 = fit_k(my_network_1, training, torch.tensor(np.concatenate((data_val[:8]),axis=1)).float(), torch.tensor(data_val[8]), n_epochs_1, optimizer, p=2, reg_param=0.0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.grid(True, which=\"both\", ls=\":\")\n",
    "if perc == 0:\n",
    "    plt.plot(np.arange(1,n_epochs_1+1), history_1, label='Training loss')\n",
    "else:\n",
    "    plt.plot(np.arange(1,n_epochs_1+1), history_1[0], label='Training loss')\n",
    "    plt.plot(np.arange(1,n_epochs_1+1), history_1[1], label='Validation loss')\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_table('TrainingData_101.txt', delimiter=' ', dtype='float', header=None)\n",
    "data_test = np.array(data_test.iloc[:,:])\n",
    "data_test_norm = []\n",
    "for i in range(data_test.shape[1]):\n",
    "    data_test_norm.append(((data_test[:,i]-data_test[:,i].mean())/data_test[:,i].std()).reshape(-1,1))\n",
    "\n",
    "cf_test = my_network_1(torch.tensor(np.concatenate(data_test_norm[:8], axis=1)).float()).reshape(-1,).detach()\n",
    "cf_test = cf_test*data_test[:,8].std() + data_test[:,8].mean()\n",
    "\n",
    "cf_train = my_network_1(torch.tensor(np.concatenate(data_train[:8], axis=1)).float()).reshape(-1,).detach()\n",
    "cf_train = cf_train*data_std[8] + data_mean[8]\n",
    "\n",
    "relative_error_train = torch.mean((cf_train - torch.tensor(data[:training_size,8]))**2)/torch.mean(torch.tensor(data[:training_size,8])**2)\n",
    "print(\"Relative Training Error: \", relative_error_train.detach().numpy()**0.5*100, \"%\")\n",
    "\n",
    "if perc != 0:\n",
    "    cf_val = my_network_1(torch.tensor(np.concatenate(data_val[:8], axis=1)).float()).reshape(-1,).detach()\n",
    "    cf_val = cf_val*data_std[8] + data_mean[8]\n",
    "    relative_error_train = torch.mean((cf_val - torch.tensor(data[training_size:,8]))**2)/torch.mean(torch.tensor(data[training_size:,8])**2)\n",
    "    print(\"Relative Validation Error: \", relative_error_train.detach().numpy()**0.5*100, \"%\")\n",
    "\n",
    "relative_error_test = torch.mean((cf_test - torch.tensor(data_test[:,8]))**2)/torch.mean(torch.tensor(data_test[:,8])**2)\n",
    "print(\"Relative Testing Error: \", relative_error_test.detach().numpy()**0.5*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot(data[:,8], label='train')\n",
    "sns.distplot(cf_train.detach(), label='pred on train')\n",
    "sns.distplot(data_test[:,8], label='test')\n",
    "sns.distplot(cf_test.detach(), label='pred on test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Sobol points to variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sobol_pts = pd.read_table('samples_sobol.txt', delimiter=' ', dtype='float', header=None)\n",
    "temp = np.zeros((len(data),sobol_pts.shape[1]))\n",
    "y = []\n",
    "y_mean, y_std = [], []\n",
    "for j in range(temp.shape[1]):\n",
    "    temp[:,j] = sobol_pts.iloc[:len(data),j]\n",
    "    y_mean.append(temp[:,j].mean())\n",
    "    y_std.append(temp[:,j].std())\n",
    "    #y.append(torch.tensor(temp[:,j]).reshape(-1,1))\n",
    "    y.append(torch.tensor((temp[:,j]-y_mean[-1])/y_std[-1]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = y[0].shape[0]\n",
    "n_epochs_2 = 500\n",
    "my_network_2, history_2 = [], []\n",
    "for t in range(temp.shape[1]):\n",
    "    training = DataLoader(torch.utils.data.TensorDataset(y[t], torch.tensor(data_norm[t])), batch_size=batch_size, shuffle=True)\n",
    "    my_network_2.append(NeuralNet_Seq(input_dimension=y[t].shape[1], output_dimension=data_norm[t].shape[1], n_hidden_layers=1, neurons=10))\n",
    "    init_xavier(my_network_2[t], retrain + np.random.randint(-retrain,retrain))\n",
    "    optimizer = optim.LBFGS(my_network_2[t].parameters(), lr=0.1, max_iter=1, max_eval=50000, tolerance_change=1.0 * np.finfo(float).eps)\n",
    "    history_2.append(fit(my_network_2[t], training, n_epochs_2, optimizer, p=2, reg_param=0.00, verbose=False))\n",
    "    print('Training variable ', t+1, 'done.')"
   ]
  },
  {
   "source": [
    "# Final prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_table('TestingData.txt', delimiter=' ', dtype='float', header=None)\n",
    "y_test = np.array(y_test.iloc[:,:])\n",
    "y_test_norm, var_pred = [], []\n",
    "for i in range(y_test.shape[1]):\n",
    "    #y_test_norm.append(torch.tensor(y_test[:,i]).reshape(-1,1))\n",
    "    y_test_norm.append(torch.tensor((y_test[:,i]-y_test[:,i].mean())/y_test[:,i].std()).reshape(-1,1))\n",
    "    var_pred.append(my_network_2[i](y_test_norm[i].float()).reshape(-1,).detach())\n",
    "    var_pred[-1] = ((var_pred[-1] - var_pred[-1].mean())/var_pred[-1].std()).reshape(-1,1)\n",
    "\n",
    "cf = my_network_1(torch.tensor(np.concatenate(var_pred,axis=1))).detach().numpy()\n",
    "cf = cf*data_std[8] + data_mean[8]\n",
    "\n",
    "df = pd.DataFrame(np.array(cf.reshape(-1,)))\n",
    "df.to_csv('Task2.txt', header = False, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.distplot(data[:,8], label='train')\n",
    "sns.distplot(data_test[:,8], label='previous test')\n",
    "sns.distplot(cf, label='final pred')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ]
}